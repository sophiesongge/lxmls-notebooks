{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Learning Structured Predictors\n",
    "\n",
    "In Day 2, we focused on generative sequence classifiers - HMMs. Today's focus is on discriminative classifiers. Recall that:\n",
    "\n",
    "* **generative classifiers** try to model the probability distribution of the data, $P(X, Y)$;\n",
    "\n",
    "* **discriminative classifiers** only model the conditional probability of each class given the observed data, $P(Y\\,|\\,X)$.\n",
    "\n",
    "In Day 1, we implemented discriminative models for classification tasks. Today, we extend this concept to the classification of _sequential_ data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You will be using two discriminative classifiers to do part-of-speech tagging:\n",
    "* Conditional Random Fields (CRF) and\n",
    "* Structured Perceptron.\n",
    "\n",
    "Your tasks for this lab session are:\n",
    "\n",
    "* to train a CRF model using two different sets of features (exercises 3.1 and 3.2); \n",
    "* to implement the structured perceptron algorithm (exercise 3.3); \n",
    "* to compare the performance of the Structured Perceptron with that of CRFs (exercise 3.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "# We will this append to ensure we can import lxmls toolking\n",
    "sys.path.append('../lxmls-toolkit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Discriminative sequence models aim to solve the following:\n",
    "\n",
    "$$\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ P(Y=y\\,|\\,X=x)=\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \\boldsymbol{w}\\cdot\\boldsymbol{f}(x, y)$$\n",
    "\n",
    "where $\\boldsymbol{w}$ is the model's weight vector, and $\\boldsymbol{f}(x, y)$ is a feature vector. Notice that now both $y$ and $x$ are $N$-dimensional vectors, whereas in Day 1, these variables were just scalar numbers.\n",
    "\n",
    "In Day 2, sequences were scored using the log-probability. On today's models we are still scoring the sequences; the only difference is the scores are now computed as the product of the weights with the feature vector:\n",
    "\n",
    "\n",
    "| score | Hidden Markov Models (Day 2) | Discriminative Models (Today) |\n",
    "| ------------------------------- | ---------------- | ---------------- |\n",
    "| $\\textrm{score}_\\textrm{emiss}$ | $\\log P(x_i\\,|\\,y_i) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i)$ |\n",
    "| $\\textrm{score}_\\textrm{init}$ | $\\log P(y_1\\,|\\,\\mathrm{start}) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1)$ |\n",
    "| $\\textrm{score}_\\textrm{trans}$ | $\\log P(y_{i+1}\\,|\\,y_i) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1})$ |\n",
    "| $\\textrm{score}_\\textrm{final}$ | $\\log P(\\mathrm{stop}\\,|\\,y_N) $ | $\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N)$ |\n",
    "\n",
    "Notice that the scores computed using the feature vector depend on two sequential values of the output variable, $y$, but may depend on the whole observated input, $x$. We can now rewrite the above expression as\n",
    "\n",
    "$$\n",
    "\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \n",
    "\\sum_{i=1}^N \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1}) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N) = \n",
    "\\\\\n",
    "\\underset{y\\,\\in\\,\\Lambda^N}{\\textrm{arg max}}\\ \n",
    "\\sum_{i=1}^N \\textrm{score}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\textrm{score}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\textrm{score}_\\textrm{trans}(i, x, y_i, y_{i+1}) +\n",
    "\\textrm{score}_\\textrm{final}(x, y_N)\n",
    "$$\n",
    "\n",
    "The advantage of these linear models over HMMs is that the feature vector may contain more general features. [TODO example of what these 'standard' and 'extended' features could be]\n",
    "\n",
    "### Decoding\n",
    "\n",
    "One important thing to notice is that the decoding process - the process by which we pick the most likely label $y_i$ for the observation $x_i$ - stays the same. This means *we do not need to develop new decoders,* only new functions to compute the scores. Because of this, we will keep using the Viterbi and Forward-Backward algorithms developed on Day 2.\n",
    "\n",
    "### Training the classifier\n",
    "\n",
    "Today we will cover two different approaches to training sequential discriminative models. Given a training set with $M$ observation-label pairs, $\\{(x_m, y_m)\\}_{m=1}^M$ (note that $x_m$, $y_m$ are $N$-dimensional vectors, as $m$ indexes the training sample):\n",
    "\n",
    "* **Conditional Random Fields** maximize the log-likelihood over $w$ on the observed training set.\n",
    "* **Structured Perceptron** iteratively updates $w$ in order to correctly classify the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Fields\n",
    "\n",
    "CRFs are the generalization of the Maximum Entropy classifier for sequences. The general concept is the same, with a couple of diferences to be discussed below. They are trained by solving the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\underset{w}{\\textrm{arg max}}\\ \\sum_{m=1}^M\\log P_w(Y=y_m\\,|\\,X=x_m)\n",
    "$$\n",
    "\n",
    "where [TODO fix parenthesis... apparently \\left( doesnt work in notebooks]\n",
    "\n",
    "$$\n",
    "P_w(Y=y_m\\,|\\,X=x_m) =\n",
    "\\frac{1}{Z(w, x)}\\ \\exp (\\sum_{i=1}^N \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1}) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N))\\\\\n",
    "Z(w, x) = \\sum_{y\\,\\in\\,\\Lambda^N} \\exp (\\sum_{i=1}^N \\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{emiss}(i, x, y_i) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{init}(x, y_1) + \n",
    "\\sum_{i=1}^{N-1}\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{trans}(i, x, y_i, y_{i+1}) + \n",
    "\\boldsymbol{w}\\cdot\\boldsymbol{f}_\\textrm{final}(x, y_N))\n",
    "$$\n",
    "\n",
    "As before, the partition function $Z(w,x)$ ensures the sum of probabilities over all possible labels $y\\in\\Lambda^N$ is equal to 1.\n",
    "\n",
    "To avoid overfitting, it is common to add the Euclidean norm function as a regularization term. This is equivalent\n",
    "to considering a zero-mean Gaussian prior on the weight vector. The optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\underset{w}{\\textrm{arg max}}\\ \\sum_{m=1}^M\\log P_w(Y=y_m\\,|\\,X=x_m) - \\frac{\\lambda}{2}||w||^2\n",
    "$$\n",
    "\n",
    "which is precisely the structured variant of the maximum entropy method discussed on Day 1. Unlike with HMMs, the above problem has to be solved numerically.\n",
    "\n",
    "### Differences with respect to ME algorithm\n",
    "\n",
    "* CRe does not compute posterior marginals, $P(Y=y\\,|\\,X=x)$ for every possible $y\\in\\Lambda^N$, as there are exponentially many possible $y$'s. Instead, it decomposes the model into parts — nodes and edges — and computes the posteriors for those parts, that is, $P(Y_i=y_i\\,|\\,X=x)$ and $P(Y_i=y_i, Y_{i+1}=y_{i+1}\\,|\\,X=x)$. The crucial point is that these quantities can be computed using the forward-backward algorithm.\n",
    "\n",
    "* Instead of updating the features for all possible outputs $y′ ∈ \\Lambda^N$, we again exploit the decomposition into parts above and update only “local features” at the nodes and edges. [TODO clarify this]\n",
    "\n",
    "### Pseudo Code\n",
    "\n",
    "Below is pseudo code to optimize a CRF with the stochastic gradient descent (SGD) algorithm. Our toolkit also includes an implementation of a quasi-Newton method, L-BFGS, which converges faster. For the purpose of this exercise, however, we will stick with SGD.\n",
    "\n",
    "[TODO pseudo code?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "​\n",
    "\n",
    "Objectives:\n",
    "\n",
    "\n",
    "* train a CRF using different feature sets for part-of-speech tagging;\n",
    "\n",
    "* evaluate the model on the training, development and test sets.\n",
    "\n",
    "\n",
    "Files used:\n",
    "\n",
    "* class CRFOnline in lxmls/sequences/crf_online.py file\n",
    "\n",
    "* class PostagCorpus in lxmls/sequences/readers/pos_corpus.py file\n",
    "\n",
    "* class IDFeatures in lxmls/sequences/id_feature.py file\n",
    "\n",
    "* class ExtendedFeatures in lxmls/sequences/extended_feature.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Feature Generation\n",
    "\n",
    "Given a dataset,\n",
    "\n",
    "in order to build the features\n",
    "\n",
    "- An instance from IDFeatures (we will call it feature_mapper) must be instanciated\n",
    "- feature_mapper.build_features() must be executed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls\n",
    "import lxmls.sequences.crf_online as crfo\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "import lxmls.sequences.id_feature as idfc\n",
    "import lxmls.sequences.extended_feature as exfc\n",
    "\n",
    "from lxmls.readers import pos_corpus\n",
    "corpus = lxmls.readers.pos_corpus.PostagCorpus()\n",
    "data_path = \"../lxmls-toolkit/data/\"\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=10, max_nr_sent=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 samples in train_seq\n"
     ]
    }
   ],
   "source": [
    "print (\"There are\", len(train_seq), \"samples in train_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ms./noun Haag/noun plays/verb Elianti/noun ./. "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper= lxmls.sequences.id_feature.IDFeatures(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will call ```feature_mapper.build_features()``` to get the features for each training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1000 samples in the features build from train_seq\n"
     ]
    }
   ],
   "source": [
    "print (\"there are\", len(feature_mapper.feature_list), \"samples in the features build from train_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0]], [[3], [5], [7], [9]], [[10]], [[1], [2], [4], [6], [8]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ms./noun Haag/noun plays/verb Elianti/noun ./. "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial features: [[0]]\n",
      "\n",
      "Transition features: [[3], [5], [7], [9]]\n",
      "\n",
      "Final features: [[10]]\n",
      "\n",
      "Emission features: [[1], [2], [4], [6], [8]]\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nInitial features:\",     feature_mapper.feature_list[0][0])\n",
    "print (\"\\nTransition features:\",  feature_mapper.feature_list[0][1])\n",
    "print (\"\\nFinal features:\",       feature_mapper.feature_list[0][2])\n",
    "print (\"\\nEmission features:\",    feature_mapper.feature_list[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codification of the features\n",
    "\n",
    "All features are saved in ``feature_mapper.feature_dict`` this represents our feature vector. If it is our feature vector why it's not a vector? Good point! In order to make the algorithm fast, the code is written using dicts, so if we access only a few positions from the dict and compute substractions it will be much faster than computing the substraction of two huge weight vectors.\n",
    "\n",
    "Features are identifyed by **init_tag:**, **prev_tag:**,  **final_prev_tag:**, **id:**\n",
    "\n",
    "- **init_tag:** when they are Initial features\n",
    "    - Example: **``init_tag:noun``** is an initial feature that describes that the first word is a noun\n",
    "    \n",
    "    \n",
    "- **prev_tag:** when they are transition features\n",
    "    - Example: **``prev_tag:noun::noun``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a noun.\n",
    "    - Example: **``prev_tag:noun:.``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a `.` (this is usually foud as the last transition feature since most phrases will end up with a dot)\n",
    "      \n",
    "\n",
    "\n",
    "- **final_prev_tag:** when they are final features\n",
    "    - Example: **``final_prev_tag:.``** is a final feature stating that the last \"word\" in the sentence was a dot.\n",
    "\n",
    "\n",
    "- **id:** when they are emission features\n",
    "    - Example: **``id:plays::verb``** is an emission feature, describing that the current word is plays and the current hidden state is a verb.\n",
    "    - Example: **``id:Feb.::noun``** is an emission feature, describing that the current word is \"Feb.\" and the current hidden state is a noun.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inv_feature_dict = {word: pos for pos, word in feature_mapper.feature_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['init_tag:noun']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prev_tag:noun::noun',\n",
       " 'prev_tag:noun::verb',\n",
       " 'prev_tag:verb::noun',\n",
       " 'prev_tag:noun::.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_prev_tag:.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id:Ms.::noun',\n",
       " 'id:Haag::noun',\n",
       " 'id:plays::verb',\n",
       " 'id:Elianti::noun',\n",
       " 'id:.::.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial features: [[11]]\n",
      "\n",
      "Transition features: [[14], [16], [5], [19], [21], [16], [24], [25]]\n",
      "\n",
      "Final features: [[10]]\n",
      "\n",
      "Emission features: [[12], [13], [15], [17], [18], [20], [22], [23], [8]]\n"
     ]
    }
   ],
   "source": [
    "print (\"\\nInitial features:\",     feature_mapper.feature_list[1][0])\n",
    "print (\"\\nTransition features:\",  feature_mapper.feature_list[1][1])\n",
    "print (\"\\nFinal features:\",       feature_mapper.feature_list[1][2])\n",
    "print (\"\\nEmission features:\",    feature_mapper.feature_list[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[11]],\n",
       " [[14], [16], [5], [19], [21], [16], [24], [25]],\n",
       " [[10]],\n",
       " [[12], [13], [15], [17], [18], [20], [22], [23], [8]]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['init_tag:det']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prev_tag:det::adj',\n",
       " 'prev_tag:adj::noun',\n",
       " 'prev_tag:noun::verb',\n",
       " 'prev_tag:verb::verb',\n",
       " 'prev_tag:verb::adj',\n",
       " 'prev_tag:adj::noun',\n",
       " 'prev_tag:noun::num',\n",
       " 'prev_tag:num::.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_prev_tag:.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[1][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id:The::det',\n",
       " 'id:new::adj',\n",
       " 'id:rate::noun',\n",
       " 'id:will::verb',\n",
       " 'id:be::verb',\n",
       " 'id:payable::adj',\n",
       " 'id:Feb.::noun',\n",
       " 'id:15::num',\n",
       " 'id:.::.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[1][3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2683"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_dict[\"id:Each::det\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'init_tag:det'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_feature_dict [11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = [inv_feature_dict[x]==\"id:Each::det\"  for x in range(len(inv_feature_dict))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946 id:Each::det\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(inv_feature_dict)):\n",
    "    if inv_feature_dict[x]==\"id:Each::det\":\n",
    "        print (x, inv_feature_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'init_tag:noun',\n",
       " 1: 'id:Ms.::noun',\n",
       " 2: 'id:Haag::noun',\n",
       " 3: 'prev_tag:noun::noun',\n",
       " 4: 'id:plays::verb',\n",
       " 5: 'prev_tag:noun::verb',\n",
       " 6: 'id:Elianti::noun',\n",
       " 7: 'prev_tag:verb::noun',\n",
       " 8: 'id:.::.',\n",
       " 9: 'prev_tag:noun::.',\n",
       " 10: 'final_prev_tag:.',\n",
       " 11: 'init_tag:det',\n",
       " 12: 'id:The::det',\n",
       " 13: 'id:new::adj',\n",
       " 14: 'prev_tag:det::adj',\n",
       " 15: 'id:rate::noun',\n",
       " 16: 'prev_tag:adj::noun',\n",
       " 17: 'id:will::verb',\n",
       " 18: 'id:be::verb',\n",
       " 19: 'prev_tag:verb::verb',\n",
       " 20: 'id:payable::adj',\n",
       " 21: 'prev_tag:verb::adj',\n",
       " 22: 'id:Feb.::noun',\n",
       " 23: 'id:15::num',\n",
       " 24: 'prev_tag:noun::num',\n",
       " 25: 'prev_tag:num::.',\n",
       " 26: 'id:A::det',\n",
       " 27: 'id:record::noun',\n",
       " 28: 'prev_tag:det::noun',\n",
       " 29: 'id:date::noun',\n",
       " 30: 'id:has::verb',\n",
       " 31: \"id:n't::adv\",\n",
       " 32: 'prev_tag:verb::adv',\n",
       " 33: 'id:been::verb',\n",
       " 34: 'prev_tag:adv::verb',\n",
       " 35: 'id:set::verb',\n",
       " 36: 'prev_tag:verb::.',\n",
       " 37: 'init_tag:adv',\n",
       " 38: 'id:Not::adv',\n",
       " 39: 'id:all::det',\n",
       " 40: 'prev_tag:adv::det',\n",
       " 41: 'id:those::det',\n",
       " 42: 'prev_tag:det::det',\n",
       " 43: 'id:who::pron',\n",
       " 44: 'prev_tag:det::pron',\n",
       " 45: 'id:wrote::verb',\n",
       " 46: 'prev_tag:pron::verb',\n",
       " 47: 'id:oppose::verb',\n",
       " 48: 'id:the::det',\n",
       " 49: 'prev_tag:verb::det',\n",
       " 50: 'id:changes::noun',\n",
       " 51: 'id:Bach::noun',\n",
       " 52: \"id:'s::prt\",\n",
       " 53: 'prev_tag:noun::prt',\n",
       " 54: 'id:``::.',\n",
       " 55: 'prev_tag:prt::.',\n",
       " 56: 'id:Air::noun',\n",
       " 57: 'prev_tag:.::noun',\n",
       " 58: \"id:''::.\",\n",
       " 59: 'id:followed::verb',\n",
       " 60: 'prev_tag:.::verb',\n",
       " 61: 'init_tag:conj',\n",
       " 62: 'id:Or::conj',\n",
       " 63: 'id:was::verb',\n",
       " 64: 'prev_tag:conj::verb',\n",
       " 65: 'id:it::pron',\n",
       " 66: 'prev_tag:verb::pron',\n",
       " 67: 'id:because::adp',\n",
       " 68: 'prev_tag:pron::adp',\n",
       " 69: 'prev_tag:adp::noun',\n",
       " 70: 'id:Collins::noun',\n",
       " 71: 'id:had::verb',\n",
       " 72: 'id:gone::verb',\n",
       " 73: 'id:?::.',\n",
       " 74: 'init_tag:verb',\n",
       " 75: 'id:Is::verb',\n",
       " 76: 'id:this::det',\n",
       " 77: 'id:future::noun',\n",
       " 78: 'id:of::adp',\n",
       " 79: 'prev_tag:noun::adp',\n",
       " 80: 'id:chamber::noun',\n",
       " 81: 'id:music::noun',\n",
       " 82: 'init_tag:pron',\n",
       " 83: 'id:What::pron',\n",
       " 84: \"id:'s::verb\",\n",
       " 85: 'id:next::adj',\n",
       " 86: 'prev_tag:adj::.',\n",
       " 87: 'id:Slides::noun',\n",
       " 88: 'id:to::prt',\n",
       " 89: 'id:illustrate::verb',\n",
       " 90: 'prev_tag:prt::verb',\n",
       " 91: 'id:Shostakovich::noun',\n",
       " 92: 'id:quartets::noun',\n",
       " 93: 'id:And::conj',\n",
       " 94: 'id:their::pron',\n",
       " 95: 'prev_tag:conj::pron',\n",
       " 96: 'id:suspicions::noun',\n",
       " 97: 'prev_tag:pron::noun',\n",
       " 98: 'id:each::det',\n",
       " 99: 'prev_tag:adp::det',\n",
       " 100: 'id:other::adj',\n",
       " 101: 'id:run::verb',\n",
       " 102: 'prev_tag:adj::verb',\n",
       " 103: 'id:deep::adv',\n",
       " 104: 'prev_tag:adv::.',\n",
       " 105: 'init_tag:.',\n",
       " 106: 'id:Old-time::adj',\n",
       " 107: 'prev_tag:.::adj',\n",
       " 108: 'id:kiddies::noun',\n",
       " 109: 'id:,::.',\n",
       " 110: 'prev_tag:.::.',\n",
       " 111: 'id:he::pron',\n",
       " 112: 'prev_tag:.::pron',\n",
       " 113: 'id:says::verb',\n",
       " 114: 'id:Perhaps::adv',\n",
       " 115: 'id:not::adv',\n",
       " 116: 'prev_tag:conj::adv',\n",
       " 117: 'id:just::adv',\n",
       " 118: 'prev_tag:adv::adv',\n",
       " 119: 'id:for::adp',\n",
       " 120: 'prev_tag:adv::adp',\n",
       " 121: 'id:players::noun',\n",
       " 122: 'id:My::pron',\n",
       " 123: 'id:fastball::noun',\n",
       " 124: 'id:is::verb',\n",
       " 125: 'id:good::adj',\n",
       " 126: 'id:I::pron',\n",
       " 127: 'id:tried::verb',\n",
       " 128: 'id:But::conj',\n",
       " 129: 'prev_tag:conj::det',\n",
       " 130: 'id:ballplayers::noun',\n",
       " 131: 'id:disagree::verb',\n",
       " 132: 'init_tag:adj',\n",
       " 133: 'id:Most::adj',\n",
       " 134: 'id:are::verb',\n",
       " 135: 'id:trim::adj',\n",
       " 136: 'id:there::det',\n",
       " 137: 'prev_tag:det::verb',\n",
       " 138: 'id:pride::noun',\n",
       " 139: 'id:So::adv',\n",
       " 140: 'prev_tag:adv::pron',\n",
       " 141: 'id:adjusts::verb',\n",
       " 142: 'id:He::pron',\n",
       " 143: 'id:no::adv',\n",
       " 144: 'prev_tag:pron::adv',\n",
       " 145: 'id:longer::adv',\n",
       " 146: 'id:crowds::verb',\n",
       " 147: 'id:plate::noun',\n",
       " 148: 'id:expect::verb',\n",
       " 149: 'id:slower::adj',\n",
       " 150: 'id:fastballs::noun',\n",
       " 151: 'id:Its::pron',\n",
       " 152: 'id:maximum::adj',\n",
       " 153: 'prev_tag:pron::adj',\n",
       " 154: 'id:velocity::noun',\n",
       " 155: 'id:72::num',\n",
       " 156: 'prev_tag:verb::num',\n",
       " 157: 'id:mph::noun',\n",
       " 158: 'prev_tag:num::noun',\n",
       " 159: 'id:worried::adj',\n",
       " 160: 'prev_tag:adv::adj',\n",
       " 161: 'id:control::noun',\n",
       " 162: 'id:Terms::noun',\n",
       " 163: 'id:were::verb',\n",
       " 164: 'id:disclosed::verb',\n",
       " 165: 'id:Tuesday::noun',\n",
       " 166: 'id:October::noun',\n",
       " 167: 'id:31::num',\n",
       " 168: 'id:1989::num',\n",
       " 169: 'prev_tag:.::num',\n",
       " 170: 'final_prev_tag:num',\n",
       " 171: 'id:PRIME::adj',\n",
       " 172: 'id:RATE::noun',\n",
       " 173: 'id::::.',\n",
       " 174: 'id:10::num',\n",
       " 175: 'id:1\\\\/2::num',\n",
       " 176: 'prev_tag:num::num',\n",
       " 177: 'id:%::noun',\n",
       " 178: 'id:DISCOUNT::noun',\n",
       " 179: 'id:7::num',\n",
       " 180: 'id:minimum::adj',\n",
       " 181: 'id:unit::noun',\n",
       " 182: 'id:$::.',\n",
       " 183: 'id:100,000::num',\n",
       " 184: 'id:Source::noun',\n",
       " 185: 'id:Telerate::noun',\n",
       " 186: 'id:Systems::noun',\n",
       " 187: 'id:Inc::noun',\n",
       " 188: 'id:MERRILL::noun',\n",
       " 189: 'id:LYNCH::noun',\n",
       " 190: 'id:READY::noun',\n",
       " 191: 'id:ASSETS::noun',\n",
       " 192: 'id:TRUST::noun',\n",
       " 193: 'id:8.63::num',\n",
       " 194: 'id:Output::noun',\n",
       " 195: 'id:goods-producing::adj',\n",
       " 196: 'prev_tag:adp::adj',\n",
       " 197: 'id:industries::noun',\n",
       " 198: 'id:increased::verb',\n",
       " 199: 'id:0.1::num',\n",
       " 200: 'id:venture::noun',\n",
       " 201: 'id:based::verb',\n",
       " 202: 'id:in::adp',\n",
       " 203: 'prev_tag:verb::adp',\n",
       " 204: 'id:Indianapolis::noun',\n",
       " 205: 'id:Mr.::noun',\n",
       " 206: 'id:Tomash::noun',\n",
       " 207: 'id:remain::verb',\n",
       " 208: 'id:as::adp',\n",
       " 209: 'id:a::det',\n",
       " 210: 'id:director::noun',\n",
       " 211: 'id:emeritus::noun',\n",
       " 212: 'id:Rubendall::noun',\n",
       " 213: 'id:could::verb',\n",
       " 214: 'id:reached::verb',\n",
       " 215: 'id:index::noun',\n",
       " 216: 'id:fell::verb',\n",
       " 217: 'id:109.85::num',\n",
       " 218: 'id:Monday::noun',\n",
       " 219: 'id:Institutional::adj',\n",
       " 220: 'id:investors::noun',\n",
       " 221: 'id:mostly::adv',\n",
       " 222: 'prev_tag:noun::adv',\n",
       " 223: 'id:remained::verb',\n",
       " 224: 'id:on::adp',\n",
       " 225: 'id:sidelines::noun',\n",
       " 226: 'id:Sumitomo::noun',\n",
       " 227: 'id:Realty::noun',\n",
       " 228: 'id:&::conj',\n",
       " 229: 'prev_tag:noun::conj',\n",
       " 230: 'id:Development::noun',\n",
       " 231: 'prev_tag:conj::noun',\n",
       " 232: 'id:rose::verb',\n",
       " 233: 'id:40::num',\n",
       " 234: 'prev_tag:num::prt',\n",
       " 235: 'id:2170::num',\n",
       " 236: 'prev_tag:prt::num',\n",
       " 237: 'id:Heiwa::noun',\n",
       " 238: 'id:Real::noun',\n",
       " 239: 'id:Estate::noun',\n",
       " 240: 'id:gained::verb',\n",
       " 241: 'id:2210::num',\n",
       " 242: 'id:Investor::noun',\n",
       " 243: 'id:focus::noun',\n",
       " 244: 'id:shifted::verb',\n",
       " 245: 'id:quickly::adv',\n",
       " 246: 'id:traders::noun',\n",
       " 247: 'id:said::verb',\n",
       " 248: 'id:No::det',\n",
       " 249: 'id:one::noun',\n",
       " 250: 'id:wants::verb',\n",
       " 251: 'id:stock::noun',\n",
       " 252: 'prev_tag:adp::pron',\n",
       " 253: 'id:books::noun',\n",
       " 254: 'id:Taipei::noun',\n",
       " 255: 'id:closed::verb',\n",
       " 256: 'id:holiday::noun',\n",
       " 257: 'id:percentage::noun',\n",
       " 258: 'id:change::noun',\n",
       " 259: 'id:since::adp',\n",
       " 260: 'id:year-end::noun',\n",
       " 261: 'id:1980::num',\n",
       " 262: 'prev_tag:adp::num',\n",
       " 263: 'id:equaling::verb',\n",
       " 264: 'prev_tag:num::verb',\n",
       " 265: 'id:100::num',\n",
       " 266: 'init_tag:adp',\n",
       " 267: 'id:For::adp',\n",
       " 268: 'id:longer-term::adj',\n",
       " 269: 'id:CDs::noun',\n",
       " 270: 'id:yields::noun',\n",
       " 271: 'id:up::prt',\n",
       " 272: 'prev_tag:verb::prt',\n",
       " 273: 'id:In::adp',\n",
       " 274: 'id:France::noun',\n",
       " 275: 'id:!::.',\n",
       " 276: 'id:(::.',\n",
       " 277: 'id:still::adv',\n",
       " 278: 'id:say::verb',\n",
       " 279: 'id:do::verb',\n",
       " 280: 'id:look::verb',\n",
       " 281: 'id:down::adv',\n",
       " 282: 'id:At::adp',\n",
       " 283: 'id:least::adj',\n",
       " 284: 'prev_tag:adj::adv',\n",
       " 285: 'id:when::adv',\n",
       " 286: 'id:you::pron',\n",
       " 287: 'id:ascending::verb',\n",
       " 288: 'id:)::.',\n",
       " 289: \"id:'m::verb\",\n",
       " 290: 'id:talking::verb',\n",
       " 291: 'id:about::adp',\n",
       " 292: 'id:landing::noun',\n",
       " 293: 'id:canal::noun',\n",
       " 294: 'id:porous::adj',\n",
       " 295: 'id:wicker::noun',\n",
       " 296: 'id:basket::noun',\n",
       " 297: 'id:With::adp',\n",
       " 298: 'id:pilot::noun',\n",
       " 299: 'prev_tag:noun::pron',\n",
       " 300: 'id:speaks::verb',\n",
       " 301: 'id:no::det',\n",
       " 302: 'id:English::noun',\n",
       " 303: 'id:wonder::noun',\n",
       " 304: 'id:We::pron',\n",
       " 305: 'id:coming::verb',\n",
       " 306: 'id:straight::adv',\n",
       " 307: 'id:into::adp',\n",
       " 308: 'id:neither::adv',\n",
       " 309: 'id:can::verb',\n",
       " 310: 'id:your::pron',\n",
       " 311: 'id:Which::det',\n",
       " 312: 'id:makes::verb',\n",
       " 313: 'id:chase::noun',\n",
       " 314: 'id:car::noun',\n",
       " 315: 'id:necessary::adj',\n",
       " 316: 'prev_tag:noun::adj',\n",
       " 317: 'id:looked::verb',\n",
       " 318: 'id:at::adp',\n",
       " 319: 'id:my::pron',\n",
       " 320: 'id:watch::noun',\n",
       " 321: 'id:Barely::adv',\n",
       " 322: 'id:half-an-hour::noun',\n",
       " 323: 'prev_tag:adv::noun',\n",
       " 324: 'id:aloft::adv',\n",
       " 325: 'id:de::noun',\n",
       " 326: 'id:Vries::noun',\n",
       " 327: 'id:free-lance::adj',\n",
       " 328: 'id:writer::noun',\n",
       " 329: 'id:Fed::noun',\n",
       " 330: 'id:spokesman::noun',\n",
       " 331: 'id:denied::verb',\n",
       " 332: 'id:LaFalce::noun',\n",
       " 333: 'id:statement::noun',\n",
       " 334: 'prev_tag:prt::noun',\n",
       " 335: 'id:board::noun',\n",
       " 336: 'id:by::adp',\n",
       " 337: 'id:one::num',\n",
       " 338: 'id:26::num',\n",
       " 339: 'id:members::noun',\n",
       " 340: 'id:issue::noun',\n",
       " 341: 'id:stickier::adj',\n",
       " 342: 'id:than::adp',\n",
       " 343: 'prev_tag:adj::adp',\n",
       " 344: 'id:seems::verb',\n",
       " 345: 'id:Defining::verb',\n",
       " 346: 'id:combat::noun',\n",
       " 347: 'id:aircraft::noun',\n",
       " 348: 'id:even::adv',\n",
       " 349: 'id:tougher::adj',\n",
       " 350: 'id:Accounting::noun',\n",
       " 351: 'id:problems::noun',\n",
       " 352: 'id:raise::verb',\n",
       " 353: 'id:more::adj',\n",
       " 354: 'id:knotty::adj',\n",
       " 355: 'prev_tag:adj::adj',\n",
       " 356: 'id:issues::noun',\n",
       " 357: 'id:Saul::noun',\n",
       " 358: 'id:Resnick::noun',\n",
       " 359: 'final_prev_tag:noun',\n",
       " 360: 'id:Vice::noun',\n",
       " 361: 'id:President::noun',\n",
       " 362: 'id:Public::noun',\n",
       " 363: 'id:Affairs::noun',\n",
       " 364: 'id:Senate::noun',\n",
       " 365: 'id:probably::adv',\n",
       " 366: 'id:vote::verb',\n",
       " 367: 'id:long::adv',\n",
       " 368: 'id:afterward::adv',\n",
       " 369: 'id:Gerald::noun',\n",
       " 370: 'id:F.::noun',\n",
       " 371: 'id:Seib::noun',\n",
       " 372: 'id:contributed::verb',\n",
       " 373: 'prev_tag:prt::det',\n",
       " 374: 'id:article::noun',\n",
       " 375: 'id:company::noun',\n",
       " 376: 'id:manufacturing::verb',\n",
       " 377: 'id:carpet::noun',\n",
       " 378: 'id:1967::num',\n",
       " 379: 'id:price::noun',\n",
       " 380: 'id:This::det',\n",
       " 381: 'id:measure::noun',\n",
       " 382: 'id:dropped::verb',\n",
       " 383: 'id:sharply::adv',\n",
       " 384: 'id:August::noun',\n",
       " 385: 'id:remainder::noun',\n",
       " 386: 'id:downturn::noun',\n",
       " 387: 'id:begin::verb',\n",
       " 388: 'id:sometime::adv',\n",
       " 389: 'final_prev_tag:adp',\n",
       " 390: 'id:Pilgrim::noun',\n",
       " 391: 'id:closed::adj',\n",
       " 392: 'id:32::num',\n",
       " 393: 'id:months::noun',\n",
       " 394: 'id:Bids::noun',\n",
       " 395: 'id:totaling::verb',\n",
       " 396: 'id:515::num',\n",
       " 397: 'id:million::num',\n",
       " 398: 'id:submitted::verb',\n",
       " 399: 'id:475::num',\n",
       " 400: 'id:It::pron',\n",
       " 401: 'id:also::adv',\n",
       " 402: 'id:unnecessary::adj',\n",
       " 403: 'id:They::pron',\n",
       " 404: 'id:promised::verb',\n",
       " 405: 'id:yet::adv',\n",
       " 406: 'id:really::adv',\n",
       " 407: 'prev_tag:adp::adv',\n",
       " 408: 'id:stuff::noun',\n",
       " 409: 'id:Finding::verb',\n",
       " 410: 'id:him::pron',\n",
       " 411: 'id:became::verb',\n",
       " 412: 'id:an::det',\n",
       " 413: 'id:obsession::noun',\n",
       " 414: 'id:Stoll::noun',\n",
       " 415: 'id:Some::det',\n",
       " 416: 'id:nights::noun',\n",
       " 417: 'id:slept::verb',\n",
       " 418: 'id:under::adp',\n",
       " 419: 'id:his::pron',\n",
       " 420: 'id:desk::noun',\n",
       " 421: 'id:His::pron',\n",
       " 422: 'id:boss::noun',\n",
       " 423: 'id:complained::verb',\n",
       " 424: 'id:neglect::noun',\n",
       " 425: 'id:chores::noun',\n",
       " 426: 'id:Finally::adv',\n",
       " 427: 'id:got::verb',\n",
       " 428: 'id:help::noun',\n",
       " 429: 'id:Tymnet::noun',\n",
       " 430: 'id:major::adj',\n",
       " 431: 'id:network::noun',\n",
       " 432: 'id:linking::verb',\n",
       " 433: 'id:computers::noun',\n",
       " 434: 'id:angry::adj',\n",
       " 435: 'id:return::noun',\n",
       " 436: 'id:Melloan::noun',\n",
       " 437: 'id:deputy::adj',\n",
       " 438: 'id:editor::noun',\n",
       " 439: 'id:Journal::noun',\n",
       " 440: 'id:UNIFIRST::noun',\n",
       " 441: 'id:Corp.::noun',\n",
       " 442: 'id:declared::verb',\n",
       " 443: 'id:2-for-1::adj',\n",
       " 444: 'id:split::noun',\n",
       " 445: 'id:dividend::noun',\n",
       " 446: 'id:five::num',\n",
       " 447: 'id:cents::noun',\n",
       " 448: 'prev_tag:noun::det',\n",
       " 449: 'id:share::noun',\n",
       " 450: 'id:Reserve::noun',\n",
       " 451: 'id:Fund::noun',\n",
       " 452: 'prev_tag:det::adp',\n",
       " 453: 'id:allegations::noun',\n",
       " 454: 'id:simply::adv',\n",
       " 455: 'id:bizarre::adj',\n",
       " 456: 'id:sharp::adj',\n",
       " 457: 'id:tack::noun',\n",
       " 458: 'id:Nobody::noun',\n",
       " 459: 'id:guts::noun',\n",
       " 460: 'id:complain::verb',\n",
       " 461: 'id:Certainly::adv',\n",
       " 462: 'id:lawyers::noun',\n",
       " 463: 'id:inquiry::noun',\n",
       " 464: 'id:soon::adv',\n",
       " 465: 'id:focused::verb',\n",
       " 466: 'id:judge::noun',\n",
       " 467: 'id:Later::adv',\n",
       " 468: 'prev_tag:.::det',\n",
       " 469: 'id:went::verb',\n",
       " 470: 'id:step::noun',\n",
       " 471: 'id:farther::adv',\n",
       " 472: 'id:again::adv',\n",
       " 473: 'id:bank::noun',\n",
       " 474: 'id:acquiesced::verb',\n",
       " 475: 'id:should::verb',\n",
       " 476: 'id:scream::noun',\n",
       " 477: 'id:William::noun',\n",
       " 478: 'id:S.::noun',\n",
       " 479: 'id:Smith::noun',\n",
       " 480: 'id:Virginia::noun',\n",
       " 481: 'id:M.W.::noun',\n",
       " 482: 'id:Gardiner::noun',\n",
       " 483: 'id:Continental::noun',\n",
       " 484: 'id:Cablevision::noun',\n",
       " 485: 'id:Inc.::noun',\n",
       " 486: 'id:--::.',\n",
       " 487: 'id:Beatrice::noun',\n",
       " 488: 'id:Co.::noun',\n",
       " 489: 'id:coupon::noun',\n",
       " 490: 'id:13::num',\n",
       " 491: 'id:3\\\\/4::num',\n",
       " 492: 'id:New::noun',\n",
       " 493: 'id:Jersey::noun',\n",
       " 494: 'id:Wastewater::noun',\n",
       " 495: 'id:Treatment::noun',\n",
       " 496: 'id:Trust::noun',\n",
       " 497: 'id:Matagorda::noun',\n",
       " 498: 'id:County::noun',\n",
       " 499: 'id:Navigation::noun',\n",
       " 500: 'id:District::noun',\n",
       " 501: 'id:No.::noun',\n",
       " 502: 'id:1::num',\n",
       " 503: 'id:Texas::noun',\n",
       " 504: 'id:Federal::noun',\n",
       " 505: 'id:Home::noun',\n",
       " 506: 'id:Loan::noun',\n",
       " 507: 'id:Mortgage::noun',\n",
       " 508: 'id:Complete::adj',\n",
       " 509: 'id:details::noun',\n",
       " 510: 'id:immediately::adv',\n",
       " 511: 'id:available::adj',\n",
       " 512: 'id:Lomas::noun',\n",
       " 513: 'id:Funding::noun',\n",
       " 514: 'id:II::noun',\n",
       " 515: 'id:J.C.::noun',\n",
       " 516: 'id:Penney::noun',\n",
       " 517: 'id:Diesel::noun',\n",
       " 518: 'id:Kiki::noun',\n",
       " 519: 'id:Co::noun',\n",
       " 520: 'id:Japan::noun',\n",
       " 521: 'id:Chugoku::noun',\n",
       " 522: 'id:Electric::noun',\n",
       " 523: 'id:Power::noun',\n",
       " 524: 'id:Fees::noun',\n",
       " 525: 'id:7\\\\/8::num',\n",
       " 526: 'id:Okobank::noun',\n",
       " 527: 'id:Finland::noun',\n",
       " 528: 'init_tag:x',\n",
       " 529: 'id:First::x',\n",
       " 530: 'prev_tag:x::.',\n",
       " 531: 'id:they::pron',\n",
       " 532: 'id:safe::adj',\n",
       " 533: 'id:Second::x',\n",
       " 534: 'id:liquid::adj',\n",
       " 535: 'id:Third::x',\n",
       " 536: 'id:offer::verb',\n",
       " 537: 'id:high::adj',\n",
       " 538: 'id:All::det',\n",
       " 539: 'id:concerns::noun',\n",
       " 540: 'id:Toronto::noun',\n",
       " 541: 'id:Rockefeller::noun',\n",
       " 542: 'id:investment::noun',\n",
       " 543: 'id:its::pron',\n",
       " 544: 'id:largest::adj',\n",
       " 545: 'id:`::.',\n",
       " 546: 'id:Frequent::adj',\n",
       " 547: 'id:Drinker::noun',\n",
       " 548: \"id:'::.\",\n",
       " 549: 'id:Offer::noun',\n",
       " 550: 'id:Stirs::verb',\n",
       " 551: 'id:Up::prt',\n",
       " 552: 'id:Spirited::adj',\n",
       " 553: 'prev_tag:prt::adj',\n",
       " 554: 'id:Debate::noun',\n",
       " 555: 'id:Chivas::noun',\n",
       " 556: 'id:Class::noun',\n",
       " 557: 'id:first::adj',\n",
       " 558: 'id:such::adj',\n",
       " 559: 'id:promotion::noun',\n",
       " 560: 'id:Goya::noun',\n",
       " 561: 'id:Concocts::verb',\n",
       " 562: 'id:Milk::noun',\n",
       " 563: 'id:Hispanic::adj',\n",
       " 564: 'id:Tastes::noun',\n",
       " 565: 'id:Jewelry::noun',\n",
       " 566: 'id:Makers::noun',\n",
       " 567: 'id:Copy::verb',\n",
       " 568: 'id:Cosmetics::noun',\n",
       " 569: 'id:Sales::noun',\n",
       " 570: 'id:Ploys::noun',\n",
       " 571: 'id:merchandise::noun',\n",
       " 572: 'id:well::x',\n",
       " 573: 'prev_tag:.::x',\n",
       " 574: 'id:fake::adj',\n",
       " 575: 'id:limits::noun',\n",
       " 576: 'id:Her::pron',\n",
       " 577: 'id:idea::noun',\n",
       " 578: 'id:bring::verb',\n",
       " 579: 'id:in::prt',\n",
       " 580: 'id:live::adj',\n",
       " 581: 'id:zoo::noun',\n",
       " 582: 'id:animals::noun',\n",
       " 583: 'id:Odds::noun',\n",
       " 584: 'id:and::conj',\n",
       " 585: 'id:Ends::noun',\n",
       " 586: 'id:cholesterol::noun',\n",
       " 587: 'prev_tag:.::adp',\n",
       " 588: 'id:course::noun',\n",
       " 589: 'id:reset::verb',\n",
       " 590: 'id:opening::adj',\n",
       " 591: 'id:arguments::noun',\n",
       " 592: 'id:today::noun',\n",
       " 593: 'id:She::pron',\n",
       " 594: 'id:now::adv',\n",
       " 595: 'id:lives::verb',\n",
       " 596: 'id:with::adp',\n",
       " 597: 'id:relatives::noun',\n",
       " 598: 'id:Alabama::noun',\n",
       " 599: 'id:Then::adv',\n",
       " 600: 'id:would::verb',\n",
       " 601: 'id:move::verb',\n",
       " 602: 'id:movement::noun',\n",
       " 603: 'id:Europe::noun',\n",
       " 604: 'id:that::det',\n",
       " 605: 'prev_tag:adv::prt',\n",
       " 606: 'id:Their::pron',\n",
       " 607: 'id:legacy::noun',\n",
       " 608: 'id:on::prt',\n",
       " 609: 'id:Yet::adv',\n",
       " 610: 'id:these::det',\n",
       " 611: 'id:purchases::noun',\n",
       " 612: 'id:misleading::adj',\n",
       " 613: 'id:South::noun',\n",
       " 614: 'id:Korea::noun',\n",
       " 615: 'id:continue::verb',\n",
       " 616: 'id:profitable::adj',\n",
       " 617: 'id:Panda::noun',\n",
       " 618: 'id:Motors::noun',\n",
       " 619: 'prev_tag:num::adj',\n",
       " 620: 'id:NUCLEAR::noun',\n",
       " 621: 'id:REACTOR::noun',\n",
       " 622: 'id:FOR::adp',\n",
       " 623: 'id:ISRAEL::noun',\n",
       " 624: 'id:two::num',\n",
       " 625: 'prev_tag:det::num',\n",
       " 626: 'id:signing::verb',\n",
       " 627: 'id:trade::noun',\n",
       " 628: 'id:agreement::noun',\n",
       " 629: 'id:Ashurst::noun',\n",
       " 630: 'prev_tag:adj::prt',\n",
       " 631: 'id:Far::noun',\n",
       " 632: 'id:East::noun',\n",
       " 633: 'id:NEW::noun',\n",
       " 634: 'id:JERSEY::noun',\n",
       " 635: 'id:MERGER::noun',\n",
       " 636: 'id:DRUG::noun',\n",
       " 637: 'id:WARS::noun',\n",
       " 638: 'id:level::noun',\n",
       " 639: 'id:did::verb',\n",
       " 640: 'id:doors::noun',\n",
       " 641: 'id:town-watching::adj',\n",
       " 642: 'id:excursions::noun',\n",
       " 643: 'id:downright::adv',\n",
       " 644: 'id:comic::adj',\n",
       " 645: 'id:Other::adj',\n",
       " 646: 'id:trips::noun',\n",
       " 647: 'id:more::adv',\n",
       " 648: 'id:productive::adj',\n",
       " 649: 'id:Why::adv',\n",
       " 650: 'id:Accord::noun',\n",
       " 651: 'id:prices::noun',\n",
       " 652: 'id:start::verb',\n",
       " 653: 'prev_tag:adp::.',\n",
       " 654: 'id:12,345::num',\n",
       " 655: 'id:George::noun',\n",
       " 656: 'id:Bush::noun',\n",
       " 657: 'id:own::adj',\n",
       " 658: 'id:Peter::noun',\n",
       " 659: 'id:Gumbel::noun',\n",
       " 660: 'id:Moscow::noun',\n",
       " 661: 'id:There::det',\n",
       " 662: 'id:better::adj',\n",
       " 663: 'id:ways::noun',\n",
       " 664: 'id:promote::verb',\n",
       " 665: 'id:cause::noun',\n",
       " 666: 'id:Here::adv',\n",
       " 667: 'id:cases::noun',\n",
       " 668: 'id:dies::verb',\n",
       " 669: 'id:maybe::adv',\n",
       " 670: 'id:TV::noun',\n",
       " 671: 'id:lose::verb',\n",
       " 672: 'id:nothing::noun',\n",
       " 673: 'id:States::noun',\n",
       " 674: 'id:following::verb',\n",
       " 675: 'id:suit::noun',\n",
       " 676: 'id:California::noun',\n",
       " 677: 'id:enacted::verb',\n",
       " 678: 'id:rights::noun',\n",
       " 679: 'id:law::noun',\n",
       " 680: 'id:1988::num',\n",
       " 681: 'id:HUGO::noun',\n",
       " 682: 'id:FELLED::verb',\n",
       " 683: 'id:vast::adj',\n",
       " 684: 'id:timberlands::noun',\n",
       " 685: 'id:BRIEFS::noun',\n",
       " 686: 'id:Industry::noun',\n",
       " 687: 'id:executives::noun',\n",
       " 688: 'id:wishing::verb',\n",
       " 689: 'id:Achenbaum::noun',\n",
       " 690: 'id:well::adv',\n",
       " 691: 'id:Cotton::noun',\n",
       " 692: 'id:Campaign::noun',\n",
       " 693: 'id:Frank::noun',\n",
       " 694: 'id:Mingo::noun',\n",
       " 695: 'id:Dies::verb',\n",
       " 696: 'id:49::num',\n",
       " 697: 'id:Clients::noun',\n",
       " 698: 'id:include::verb',\n",
       " 699: 'id:Miller::noun',\n",
       " 700: 'id:Brewing::noun',\n",
       " 701: 'id:General::noun',\n",
       " 702: 'id:Ad::noun',\n",
       " 703: 'id:Notes::noun',\n",
       " 704: 'id:...::.',\n",
       " 705: 'id:EARNINGS::noun',\n",
       " 706: 'id:Excerpts::noun',\n",
       " 707: 'id:follow::verb',\n",
       " 708: 'id:already::adv',\n",
       " 709: 'id:industrialized::adj',\n",
       " 710: 'id:zero-sum::adj',\n",
       " 711: 'id:game::noun',\n",
       " 712: 'id:That::det',\n",
       " 713: 'id:possible::adj',\n",
       " 714: 'id:On::adp',\n",
       " 715: 'id:U.S.-Japan::adj',\n",
       " 716: 'id:relations::noun',\n",
       " 717: 'id:encouraged::adj',\n",
       " 718: 'id:understand::verb',\n",
       " 719: 'prev_tag:pron::.',\n",
       " 720: 'id:relationships::noun',\n",
       " 721: 'id:British::noun',\n",
       " 722: 'id:totally::adv',\n",
       " 723: 'id:different::adj',\n",
       " 724: 'id:lorded::verb',\n",
       " 725: 'id:over::adp',\n",
       " 726: 'id:me::pron',\n",
       " 727: 'id:some::det',\n",
       " 728: 'prev_tag:pron::det',\n",
       " 729: 'id:themselves::pron',\n",
       " 730: 'id:mean::verb',\n",
       " 731: 'id:normal::adj',\n",
       " 732: 'id:adult::noun',\n",
       " 733: 'id:relationship::noun',\n",
       " 734: 'id:trouble::noun',\n",
       " 735: 'id:over::prt',\n",
       " 736: 'id:ca::verb',\n",
       " 737: 'id:way::noun',\n",
       " 738: 'id:forward::adv',\n",
       " 739: 'id:Let::verb',\n",
       " 740: \"id:'s::pron\",\n",
       " 741: 'id:put::verb',\n",
       " 742: 'id:bluntly::adv',\n",
       " 743: 'id:First::noun',\n",
       " 744: 'id:Boston::noun',\n",
       " 745: 'id:sole::adj',\n",
       " 746: 'id:underwriter::noun',\n",
       " 747: 'id:trust::noun',\n",
       " 748: 'id:issue::verb',\n",
       " 749: 'id:certificates::noun',\n",
       " 750: 'id:service::verb',\n",
       " 751: 'id:receivables::noun',\n",
       " 752: 'id:Mexico::noun',\n",
       " 753: 'id:urgently::adv',\n",
       " 754: 'id:needs::verb',\n",
       " 755: 'id:Salinas::noun',\n",
       " 756: 'id:big::adj',\n",
       " 757: 'id:inflows::noun',\n",
       " 758: 'prev_tag:.::adv',\n",
       " 759: 'id:If::adp',\n",
       " 760: 'id:Opinion::noun',\n",
       " 761: 'id:mixed::adj',\n",
       " 762: 'id:three-month::adj',\n",
       " 763: 'id:prospects::noun',\n",
       " 764: 'id:Estimated::adj',\n",
       " 765: 'id:volume::noun',\n",
       " 766: 'id:3.5::num',\n",
       " 767: 'id:ounces::noun',\n",
       " 768: 'id:Edelson::noun',\n",
       " 769: 'id:comment::noun',\n",
       " 770: 'id:lot::noun',\n",
       " 771: 'id:needed::verb',\n",
       " 772: 'id:done::verb',\n",
       " 773: 'id:security::noun',\n",
       " 774: 'id:business::noun',\n",
       " 775: 'id:favorite::adj',\n",
       " 776: 'id:subject::noun',\n",
       " 777: 'id:love::verb',\n",
       " 778: 'id:Officials::noun',\n",
       " 779: 'id:Temple::noun',\n",
       " 780: 'id:declined::verb',\n",
       " 781: 'id:comment::verb',\n",
       " 782: 'id:FHA::noun',\n",
       " 783: 'id:program::noun',\n",
       " 784: 'id:hemorrhaging::verb',\n",
       " 785: 'id:bad::adj',\n",
       " 786: 'id:loans::noun',\n",
       " 787: 'id:Gillette::noun',\n",
       " 788: 'id:Africa::noun',\n",
       " 789: 'id:employs::verb',\n",
       " 790: 'id:about::adv',\n",
       " 791: 'id:250::num',\n",
       " 792: 'prev_tag:adv::num',\n",
       " 793: 'id:people::noun',\n",
       " 794: 'id:vote::noun',\n",
       " 795: 'id:approve::verb',\n",
       " 796: 'final_prev_tag:verb',\n",
       " 797: 'id:Trial::noun',\n",
       " 798: 'id:Terror::noun',\n",
       " 799: 'id:Arnold::noun',\n",
       " 800: 'id:J.::noun',\n",
       " 801: 'id:Zarett::noun',\n",
       " 802: 'id:Rodeo::noun',\n",
       " 803: 'id:applause::noun',\n",
       " 804: 'id:broncs::noun',\n",
       " 805: 'id:cheer::noun',\n",
       " 806: 'id:Marvin::noun',\n",
       " 807: 'id:Alisky::noun',\n",
       " 808: 'id:bottom-line::adj',\n",
       " 809: 'id:administration::noun',\n",
       " 810: 'id:lacks::verb',\n",
       " 811: 'id:comprehensive::adj',\n",
       " 812: 'id:health-care::noun',\n",
       " 813: 'id:policy::noun',\n",
       " 814: 'id:expected::verb',\n",
       " 815: 'id:report::verb',\n",
       " 816: 'id:summer::noun',\n",
       " 817: 'id:1991::num',\n",
       " 818: 'prev_tag:conj::num',\n",
       " 819: 'id:window::noun',\n",
       " 820: 'id:action::noun',\n",
       " 821: 'id:pressure::noun',\n",
       " 822: 'id:rise::verb',\n",
       " 823: 'id:costs::noun',\n",
       " 824: 'id:Limiting::verb',\n",
       " 825: 'id:care::noun',\n",
       " 826: 'id:wo::verb',\n",
       " 827: 'id:easy::adj',\n",
       " 828: 'id:or::conj',\n",
       " 829: 'prev_tag:adj::conj',\n",
       " 830: 'id:popular::adj',\n",
       " 831: 'prev_tag:conj::adj',\n",
       " 832: 'id:AFL-CIO::noun',\n",
       " 833: 'id:embraces::verb',\n",
       " 834: 'id:treatment::noun',\n",
       " 835: 'id:guidelines::noun',\n",
       " 836: 'id:steelmaker::noun',\n",
       " 837: 'id:16,000::num',\n",
       " 838: 'id:12.3::num',\n",
       " 839: 'id:shares::noun',\n",
       " 840: 'id:outstanding::adj',\n",
       " 841: 'id:Consolidation::noun',\n",
       " 842: 'id:overdue::adj',\n",
       " 843: 'id:such::det',\n",
       " 844: 'id:combination::noun',\n",
       " 845: 'id:presents::verb',\n",
       " 846: 'id:great::adj',\n",
       " 847: 'id:risks::noun',\n",
       " 848: 'id:days::noun',\n",
       " 849: 'id:respond::verb',\n",
       " 850: 'id:Nekoosa::noun',\n",
       " 851: 'id:incorporated::verb',\n",
       " 852: 'id:Maine::noun',\n",
       " 853: 'id:International::noun',\n",
       " 854: 'id:Paper::noun',\n",
       " 855: 'id:Weyerhaeuser::noun',\n",
       " 856: 'id:Bond::noun',\n",
       " 857: 'id:edged::verb',\n",
       " 858: 'id:higher::adv',\n",
       " 859: 'id:what::pron',\n",
       " 860: 'id:happened::verb',\n",
       " 861: 'id:yesterday::noun',\n",
       " 862: 'id:market::noun',\n",
       " 863: 'id:activity::noun',\n",
       " 864: 'id:Stock::noun',\n",
       " 865: 'id:rallied::verb',\n",
       " 866: 'id:active::adj',\n",
       " 867: 'id:trading::noun',\n",
       " 868: 'id:dollar::noun',\n",
       " 869: 'id:against::adp',\n",
       " 870: 'id:most::adj',\n",
       " 871: 'id:foreign::adj',\n",
       " 872: 'id:currencies::noun',\n",
       " 873: 'id:Nasdaq::noun',\n",
       " 874: 'id:7.08::num',\n",
       " 875: 'id:445.23::num',\n",
       " 876: 'id:Intel::noun',\n",
       " 877: 'id:up::adv',\n",
       " 878: 'id:3\\\\/8::num',\n",
       " 879: 'id:33::num',\n",
       " 880: 'id:Bank::noun',\n",
       " 881: 'id:Index::noun',\n",
       " 882: 'id:0.17::num',\n",
       " 883: 'id:432.78::num',\n",
       " 884: 'id:Gen-Probe::noun',\n",
       " 885: 'id:another::det',\n",
       " 886: 'id:takeover::noun',\n",
       " 887: 'id:MCI::noun',\n",
       " 888: 'id:Communications::noun',\n",
       " 889: 'id:added::verb',\n",
       " 890: 'id:43::num',\n",
       " 891: 'id:value::noun',\n",
       " 892: 'id:transaction::noun',\n",
       " 893: 'id:Santa::noun',\n",
       " 894: 'id:Fe::noun',\n",
       " 895: 'id:worth::adp',\n",
       " 896: 'id:analysts::noun',\n",
       " 897: 'id:bullish::adj',\n",
       " 898: 'id:Business::noun',\n",
       " 899: 'id:Railroad::noun',\n",
       " 900: 'id:natural::adj',\n",
       " 901: 'id:resources::noun',\n",
       " 902: 'id:real::adj',\n",
       " 903: 'id:estate::noun',\n",
       " 904: 'id:Year::noun',\n",
       " 905: 'id:ended::verb',\n",
       " 906: 'id:Dec.::noun',\n",
       " 907: 'id:Revenue::noun',\n",
       " 908: 'id:3.14::num',\n",
       " 909: 'id:billion::num',\n",
       " 910: 'id:Third::adj',\n",
       " 911: 'id:quarter::noun',\n",
       " 912: 'id:Sept.::noun',\n",
       " 913: 'id:30::num',\n",
       " 914: 'id:Average::adj',\n",
       " 915: 'id:daily::adj',\n",
       " 916: 'id:344,354::num',\n",
       " 917: 'id:widely::adv',\n",
       " 918: 'id:that::adp',\n",
       " 919: 'id:Second-tier::adj',\n",
       " 920: 'id:companies::noun',\n",
       " 921: 'id:receiving::verb',\n",
       " 922: 'id:less::adj',\n",
       " 923: 'id:per::adp',\n",
       " 924: 'id:ton::noun',\n",
       " 925: 'id:Commercial::adj',\n",
       " 926: 'id:gene-splicing::noun',\n",
       " 927: 'id:born::verb',\n",
       " 928: 'id:Armstrong::noun',\n",
       " 929: 'id:1\\\\/8::num',\n",
       " 930: 'id:39::num',\n",
       " 931: 'id:ERC::noun',\n",
       " 932: 'id:12::num',\n",
       " 933: 'id:Ogden::noun',\n",
       " 934: 'id:1\\\\/4::num',\n",
       " 935: 'id:Volume::noun',\n",
       " 936: 'id:totaled::verb',\n",
       " 937: 'id:11,820,000::num',\n",
       " 938: 'id:simultaneous::adj',\n",
       " 939: 'id:announcement::noun',\n",
       " 940: 'id:made::verb',\n",
       " 941: 'id:Big::noun',\n",
       " 942: 'id:Board::noun',\n",
       " 943: 'id:officials::noun',\n",
       " 944: 'id:publicly::adv',\n",
       " 945: 'id:work::verb',\n",
       " 946: 'id:Each::det',\n",
       " 947: 'id:agenda::noun',\n",
       " 948: \"id:'ve::verb\",\n",
       " 949: 'id:dictation::noun',\n",
       " 950: 'id:wrecking::verb',\n",
       " 951: 'prev_tag:adp::verb',\n",
       " 952: 'id:them::pron',\n",
       " 953: 'id:exchange::noun',\n",
       " 954: 'id:take::verb',\n",
       " 955: 'id:pro-active::adj',\n",
       " 956: 'id:position::noun',\n",
       " 957: 'id:Craig::noun',\n",
       " 958: 'id:Torres::noun',\n",
       " 959: 'id:bond::noun',\n",
       " 960: 'id:higher::adj',\n",
       " 961: 'id:Health-insurance::noun',\n",
       " 962: 'id:soared::verb',\n",
       " 963: 'id:Markets::noun',\n",
       " 964: 'id:Stocks::noun',\n",
       " 965: 'id:176,100,000::num',\n",
       " 966: 'id:Corporate::adj',\n",
       " 967: 'id:bonds::noun',\n",
       " 968: 'id:unchanged::adj',\n",
       " 969: 'id:Treasury::noun',\n",
       " 970: 'id:Securities::noun',\n",
       " 971: 'id:light::adj',\n",
       " 972: 'id:Short-term::adj',\n",
       " 973: 'id:rates::noun',\n",
       " 974: 'id:Issues::noun',\n",
       " 975: 'id:Carmichael::noun',\n",
       " 976: 'id:demanded::verb',\n",
       " 977: 'id:stricter::adj',\n",
       " 978: 'id:convenants::noun',\n",
       " 979: 'id:Mortgage-Backed::adj',\n",
       " 980: 'id:Municipal::adj',\n",
       " 981: 'id:Foreign::adj',\n",
       " 982: 'id:Bonds::noun',\n",
       " 983: 'id:government::noun',\n",
       " 984: 'id:markets::noun',\n",
       " 985: 'id:quiet::adj',\n",
       " 986: 'id:Japanese::adj',\n",
       " 987: 'id:showed::verb',\n",
       " 988: 'id:little::adj',\n",
       " 989: 'id:GASB::noun',\n",
       " 990: 'id:rules::noun',\n",
       " 991: 'id:apply::verb',\n",
       " 992: 'id:units::noun',\n",
       " 993: 'id:valued::verb',\n",
       " 994: 'id:800::num',\n",
       " 995: 'id:Joseph::noun',\n",
       " 996: 'id:Granville::noun',\n",
       " 997: 'id:expects::verb',\n",
       " 998: 'id:Stovall::noun',\n",
       " 999: 'id:does::verb',\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Exercise 3.1 - Basic feature set\n",
    "\n",
    "_Start by training the model. You will receive feedback when each epoch is finished. Note that running the 20 epochs might take a while._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences\n",
    "import lxmls.sequences.crf_online as crfo\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "import lxmls.sequences.id_feature as idfc\n",
    "import lxmls.sequences.extended_feature as exfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"../lxmls-toolkit/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the corpus\n",
    "corpus = pcc.PostagCorpus()\n",
    "\n",
    "# Load the training, test and development sequences\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=10, max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(data_path + \"/test-23.conll\",\n",
    "                                           max_sent_len=10, max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(data_path + \"/dev-22.conll\", \n",
    "                                          max_sent_len=10, max_nr_sent=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ms./noun Haag/noun plays/verb Elianti/noun ./. "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lxmls.sequences.sequence.Sequence"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 40, 43, 44, 41]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 6, 0, 4]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build features\n",
    "feature_mapper = lxmls.sequences.id_feature.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Objective value: -5.779018\n",
      "Epoch: 1 Objective value: -3.192724\n",
      "Epoch: 2 Objective value: -2.717537\n",
      "Epoch: 3 Objective value: -2.436614\n",
      "Epoch: 4 Objective value: -2.240491\n",
      "Epoch: 5 Objective value: -2.091833\n",
      "Epoch: 6 Objective value: -1.973353\n",
      "Epoch: 7 Objective value: -1.875643\n",
      "Epoch: 8 Objective value: -1.793034\n",
      "Epoch: 9 Objective value: -1.721857\n",
      "Epoch: 10 Objective value: -1.659605\n",
      "Epoch: 11 Objective value: -1.604499\n",
      "Epoch: 12 Objective value: -1.555229\n",
      "Epoch: 13 Objective value: -1.510806\n",
      "Epoch: 14 Objective value: -1.470468\n",
      "Epoch: 15 Objective value: -1.433612\n",
      "Epoch: 16 Objective value: -1.399759\n",
      "Epoch: 17 Objective value: -1.368518\n",
      "Epoch: 18 Objective value: -1.339566\n",
      "Epoch: 19 Objective value: -1.312636\n"
     ]
    }
   ],
   "source": [
    "# Build features\n",
    "feature_mapper = idfc.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "# Train the model\n",
    "# You will receive feedback when each epoch is finished.\n",
    "# Note that running the 20 epochs might take a while.\n",
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will receive feedback when each epoch is finished, note that running the 20 epochs might take a while. You whould get the following list:\n",
    "\n",
    "    Epoch: 0 Objective value: -5.779018\n",
    "    Epoch: 1 Objective value: -3.192724\n",
    "    Epoch: 2 Objective value: -2.717537\n",
    "    Epoch: 3 Objective value: -2.436614\n",
    "    Epoch: 4 Objective value: -2.240491\n",
    "    Epoch: 5 Objective value: -2.091833\n",
    "    Epoch: 6 Objective value: -1.973353\n",
    "    Epoch: 7 Objective value: -1.875643\n",
    "    Epoch: 8 Objective value: -1.793034\n",
    "    Epoch: 9 Objective value: -1.721857\n",
    "    Epoch: 10 Objective value: -1.659605\n",
    "    Epoch: 11 Objective value: -1.604499\n",
    "    Epoch: 12 Objective value: -1.555229\n",
    "    Epoch: 13 Objective value: -1.510806\n",
    "    Epoch: 14 Objective value: -1.470468\n",
    "    Epoch: 15 Objective value: -1.433612\n",
    "    Epoch: 16 Objective value: -1.399759\n",
    "    Epoch: 17 Objective value: -1.368518\n",
    "    Epoch: 18 Objective value: -1.339566\n",
    "    Epoch: 19 Objective value: -1.312636\n",
    "\n",
    "\n",
    "After training is done, evaluate the learned model on the training, development and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF - ID Features Accuracy Train: 0.949 Dev: 0.846 Test: 0.858\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "\n",
    "# Evaluate and print accuracies\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)\n",
    "print (\"CRF - ID Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your output should be similar to this:_\n",
    "\n",
    "_CRF - ID Features Accuracy Train: 0.949 Dev: 0.846 Test: 0.858_\n",
    "\n",
    "_Compare with the results achieved with the HMM model (0.837 on the test set). Even when using a similar feature set, a CRF yields better results than the HMM from the previous lecture._\n",
    "\n",
    "_Perform some error analysis and figure out what are the main errors the tagger is making. Compare them with the errors made by the HMM model._\n",
    "\n",
    "_**Hint:** use the methods developed in the previous lecture to help you with the error analysis._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2 - Extended feature set\n",
    "\n",
    "#### Begin Ex 3.2 ----------------------------------------------------------------------------\n",
    "\n",
    "_Train the model again, this time using the extended feature set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Objective value: -7.141596\n",
      "Epoch: 1 Objective value: -1.807511\n",
      "Epoch: 2 Objective value: -1.218877\n",
      "Epoch: 3 Objective value: -0.955739\n",
      "Epoch: 4 Objective value: -0.807821\n",
      "Epoch: 5 Objective value: -0.712858\n",
      "Epoch: 6 Objective value: -0.647382\n",
      "Epoch: 7 Objective value: -0.599442\n",
      "Epoch: 8 Objective value: -0.562584\n",
      "Epoch: 9 Objective value: -0.533411\n",
      "Epoch: 10 Objective value: -0.509885\n",
      "Epoch: 11 Objective value: -0.490548\n",
      "Epoch: 12 Objective value: -0.474318\n",
      "Epoch: 13 Objective value: -0.460438\n",
      "Epoch: 14 Objective value: -0.448389\n",
      "Epoch: 15 Objective value: -0.437800\n",
      "Epoch: 16 Objective value: -0.428402\n",
      "Epoch: 17 Objective value: -0.419990\n",
      "Epoch: 18 Objective value: -0.412406\n",
      "Epoch: 19 Objective value: -0.405524\n"
     ]
    }
   ],
   "source": [
    "# Build features\n",
    "feature_mapper = exfc.ExtendedFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "# Train the model\n",
    "# You will receive feedback when each epoch is finished.\n",
    "# Note that running the 20 epochs might take a while.\n",
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_And compute its accuracy._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF - ID Features Accuracy Train: 0.984 Dev: 0.899 Test: 0.894\n"
     ]
    }
   ],
   "source": [
    "# Make predictions for the various sequences using the trained model.\n",
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "\n",
    "# Evaluate and print accuracies\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)\n",
    "print (\"CRF - ID Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train,eval_dev, eval_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your output should be similar to this:_\n",
    "\n",
    "_CRF - Extended Features Accuracy Train: 0.984 Dev: 0.899 Test: 0.894_\n",
    "\n",
    "_Compare the errors obtained with the two different feature sets. Do some error analysis: what errors were correct by using more features? Can you think of other features to use to solve the errors you found?_\n",
    "\n",
    "#### End Ex 3.2   ----------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The main lesson from this exercise is that, if you are not satisfied by the accuracy of your algorithm, you can perform some error analysis and find out which errors your algorithm is making. You can then add more features which attempt to improve those specific errors — this is known as feature engineering.**\n",
    "\n",
    "Adding engineered features can lead to two problems:\n",
    "* More features will make training and decoding more expensive. For example, if you add features that depend on the current word and the previous word, the number of new features is the square of the number of different words, which is quite large. For example, the Penn Treebank has around 40000 different words, so you are adding a lot of new features, even though not all pairs of words will ever occur. Features that depend on three words (previous, current, and next) are even more numerous.\n",
    "\n",
    "* If features are very specific, such as the (previous word, current word, next word) one just mentioned, they might occur very rarely in the training set, which leads to overfit problems. Some of these problems (not all) can be mitigated with techniques such as smoothing, which you already learned about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3 - Algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4 - POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
