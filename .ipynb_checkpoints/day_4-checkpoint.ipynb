{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Syntax and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/davidbuchacaprats/Dropbox/lxmls2015/lxmls-toolkit-student')\n",
    "import lxmls\n",
    "import scipy\n",
    "path_inside_lxmls_toolkit_student = \"/Users/davidbuchacaprats/Dropbox/lxmls2015/lxmls-toolkit-student\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic definitions for the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition: Context-free grammar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A context-free grammar (CFG) is a tuple $ G= <\\mathcal{N}, \\mathcal{T}, \\mathcal{R}, \\mathcal{S}>$ where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- N is a finite set of non-terminal symbols. Elements of N are denoted by upper case letters (A, B, C, ...). Each non-terminal symbol is a syntactic category: it represents a different type of phrase or clause in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - T is a finite set of terminal symbols (disjoint from N). Elements of $\\mathcal{T}$ are denoted by lower case letters $(a, b, c, . . .)$. Each terminal symbol is a surface word: terminal symbols make up the actual content of sentences. The set $\\mathcal{T}$ is called the alphabet of the language defined by the grammar $\\mathcal{G}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathcal{R}$ is a set of \n",
    "production rules, i.e., a finite relation \n",
    "from $\\mathcal{N}$ to $(\\mathcal{N} \\cup \\mathcal{T})^*$. \n",
    "$G$ is said to be in Chomsky normal form (CNF) if production rules in $\\mathcal{R}$ are either of the form \n",
    "$A \\rightarrow B C$ or $A \\rightarrow a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $\\mathcal{S}$ is a \"start symbol\", used to represent the whole sentence. It must be an element of $\\mathcal{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any CFG can be transformed to be in CNF without loosing any expressive power in terms of the language it generates. Hence, we henceforth assume that G is in CNF without loss of generality.\n",
    "\n",
    "Any CFG can be transformed to be in CNF without loosing any expressive power in terms of the language it generates. Hence, we henceforth assume that G is in CNF without loss of generality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple exercise, you will see the CKY algorithm in action. \n",
    "\n",
    "There is a Javascript applet that illustrates how CKY works (in its non-probabilistic form). Go to http://lxmls.it.pt/2015/cky.html, and observe carefully the several steps taken by the algorithm. Write down a small grammar in CNF that yields multiple parses for the ambiguous sentence The man saw the boy in the park with a telescope, and run the demo for this particular sentence. \n",
    "\n",
    "What would happen in the probabilistic form of CKY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will show you that real-world sentences can have complicated syntactic structures. There is a parse tree visualizer in ```http://www.ark.cs.cmu.edu/treeviz/```. Go to your local data/treebanks folder and open the file PTB excerpt.txt. Copy a few trees from the file, one at a time, and examine their parse trees in the visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this exercise you are going to experiment with arc-factored non-projective dependency parsers.\n",
    "The CoNLL-X and CoNLL 2008 shared task datasets (Buchholz and Marsi, 2006; Surdeanu et al., 2008) contain dependency treebanks for 14 languages. In this lab, we are going to experiment with the Portuguese and English datasets.\n",
    "We preprocessed those datasets to exclude all sentences with more than 15 words; this yielded the files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data/deppars/portuguese train.conll,\n",
    "- data/deppars/portuguese test.conll,\n",
    "- data/deppars/english train.conll,\n",
    "- data/deppars/english test.conll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) After importing all the necessary libraries, load the Portuguese dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3029\n",
      "Number of tokens: 25015\n",
      "Number of words: 7621\n",
      "Number of pos: 16\n",
      "Number of features: 142\n"
     ]
    }
   ],
   "source": [
    "import lxmls.parsing.dependency_parser as depp\n",
    "dp = depp.DependencyParser()\n",
    "dp.read_data(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the statistics which are shown. How many features are there in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We will now have a close look on the features that can be used in the parser. Examine the file:\n",
    "\n",
    "            ```lxmls/parsing/dependency features.py.```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method takes a sentence and computes a \n",
    "vector of features for each possible arc\n",
    "\n",
    "```<h, m>```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    def create_arc_features(self, instance, h, m, add=False):\n",
    "        '''Creates features for arc h-->m.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grouped the features in several subsets, so that we can conduct some ablation experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic features that look only at the parts-of-speech of the words that can be connected by an arc\n",
    "\n",
    "- Lexical features that also look at these words themselves;\n",
    "\n",
    "- Distance features that look at the length and direction of the dependency link (i.e., distance between the two words);\n",
    "\n",
    "- Contextual features that look at the context (part-of-speech tags) of the words surrounding h and m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the default configuration, only the basic features are enabled. The total number of features is the quantity observed in the previous question. With this configuration, train the parser by running 10 epochs of the structured perceptron algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3029\n",
      "Number of tokens: 25015\n",
      "Number of words: 7621\n",
      "Number of pos: 16\n",
      "Number of features: 142\n",
      "Epoch 1\n",
      "Training accuracy: 0.497432605905\n",
      "Epoch 2\n",
      "Training accuracy: 0.499144201968\n",
      "Epoch 3\n",
      "Training accuracy: 0.498217087434\n",
      "Epoch 4\n",
      "Training accuracy: 0.50053487377\n",
      "Epoch 5\n",
      "Training accuracy: 0.501818570817\n",
      "Epoch 6\n",
      "Training accuracy: 0.498538011696\n",
      "Epoch 7\n",
      "Training accuracy: 0.500962772786\n",
      "Epoch 8\n",
      "Training accuracy: 0.500285266011\n",
      "Epoch 9\n",
      "Training accuracy: 0.499286834974\n",
      "Epoch 10\n",
      "Training accuracy: 0.500035658251\n"
     ]
    }
   ],
   "source": [
    "import lxmls.parsing.dependency_parser as depp\n",
    "dp = depp.DependencyParser()\n",
    "dp.read_data(\"portuguese\")\n",
    "dp.train_perceptron(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DependencyParser.test of <lxmls.parsing.dependency_parser.DependencyParser instance at 0x10eaf9950>>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy obtained in the test set? (Note: the shown accuracy is the fraction of words whose parent was correctly predicted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Repeat the previous exercise by subsequently enabling the lexical, distance and contextual features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3029\n",
      "Number of tokens: 25015\n",
      "Number of words: 7621\n",
      "Number of pos: 16\n",
      "Number of features: 46216\n",
      "Epoch 1\n",
      "Training accuracy: 0.531914134931\n",
      "Epoch 2\n",
      "Training accuracy: 0.641135358722\n",
      "Epoch 3\n",
      "Training accuracy: 0.722864070746\n",
      "Epoch 4\n",
      "Training accuracy: 0.784695478534\n",
      "Epoch 5\n",
      "Training accuracy: 0.820425046356\n",
      "Epoch 6\n",
      "Training accuracy: 0.851911282271\n",
      "Epoch 7\n",
      "Training accuracy: 0.873876765083\n",
      "Epoch 8\n",
      "Training accuracy: 0.890850092711\n",
      "Epoch 9\n",
      "Training accuracy: 0.897054628441\n",
      "Epoch 10\n",
      "Training accuracy: 0.907466837826\n",
      "Test accuracy (109 test instances): 0.57662835249\n"
     ]
    }
   ],
   "source": [
    "dp.features.use_lexical = True \n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10) \n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.features.use_distance = True\n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10) \n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.features.use_contextual = True \n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each configuration, write down the number of features and test set accuracies. Observe the improvements obtained when more features were added.\n",
    "Feel free to engineer new features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Which of the three important inference tasks discussed above (computing the most likely tree, computing the parti- tion function, and computing the marginals) need to be performed in the structured perceptron algorithm? What about a maximum entropy classifier, with stochastic gradient descent? Check your answers by looking at the fol- lowing two methods in code/dependency parser.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def train_perceptron(self, n_epochs): ...\n",
    "   \n",
    "    def train_crf_sgd(self, n_epochs, sigma, eta0 = 0.001): ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the last exercise by training a maximum entropy classifier, with stochastic gradient descent, using l = 0.01 and a initial stepsize of $\\eta_0$ = 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.train_crf_sgd(10, 0.01, 0.1)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results with those obtained by the perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Train a parser for English using your favourite learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.read_data(\"english\")\n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted trees are placed in the file ```data/deppars/english_test.conll.pred```.\n",
    "To get a sense of which errors are being made, you can check the sentences that differ\n",
    "from the gold standard (see the data in ```data/deppars/english_test.conll```) and visualize those sentences, e.g., \n",
    "in http://www.ark.cs.cmu.edu/treeviz/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) (Optional.) Implement Eisner’s algorithm for projective dependency parsing. The pseudo-code is shown as Algorithm 13. Implement this algorithm as the function:\n",
    "\n",
    "    def parse_proj(self, scores):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in file dependency decoder.py. The input is a matrix of arc scores, whose dimension is(N + 1)-by-(N + 1), and whose (h, m) entry contains the score sq(h, m). \n",
    "\n",
    "In particular, the first row contains the scores for the arcs that depart from the root, and the first column’s values, along with the main diagonal, are to be ignored (since no arcs point to the root, and there are no self-pointing arcs). To make your job easier, we provide an implementation of the backtracking part:\n",
    "\n",
    "    def backtrack_eisner(self, incomplete_backtrack, complete_backtrack, s, t, direction, complete, heads):\n",
    "    \n",
    "so you just need to build complete/incomplete spans and their backtrack pointers and then call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    heads = -np.ones(N+1, dtype=int) \n",
    "    self.backtrack_eisner(incomplete_backtrack, complete_backtrack, 0, N, 1, 1,heads)\n",
    "    return heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to obtain the final parse.\n",
    "To test the algorithm, retrain the parser on the English data (where the trees are actually all projective) by setting\n",
    "the flag dp.projective to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser() \n",
    "dp.features.use_lexical = True \n",
    "dp.features.use_distance = True \n",
    "dp.features.use_contextual = True \n",
    "dp.read_data(\"english\") \n",
    "dp.projective = True\n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼4.2.5\n",
    "    Number of sentences: 8044\n",
    "    Number of tokens: 80504\n",
    "    Number of words: 12202\n",
    "    Number of pos: 48\n",
    "    Number of features: 338014\n",
    "    Epoch 1\n",
    "    Training accuracy: 0.835637168541\n",
    "    Epoch 2\n",
    "    Training accuracy: 0.922426254687\n",
    "    Epoch 3\n",
    "    Training accuracy: 0.947621628947\n",
    "    Epoch 4\n",
    "    Training accuracy: 0.960326602521\n",
    "    Epoch 5\n",
    "    Training accuracy: 0.967689840538\n",
    "    Epoch 6\n",
    "    Training accuracy: 0.97263631025\n",
    "    Epoch 7\n",
    "    Training accuracy: 0.97619370285\n",
    "    Epoch 8\n",
    "    Training accuracy: 0.979209016579\n",
    "    Epoch 9\n",
    "    Training accuracy: 0.98127569228\n",
    "    Epoch 10\n",
    "    Training accuracy: 0.981320865519\n",
    "    Test accuracy (509 test instances): 0.886732599366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {}
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
